{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056},{"sourceId":4853613,"sourceType":"datasetVersion","datasetId":2813430},{"sourceId":10972432,"sourceType":"datasetVersion","datasetId":6822545},{"sourceId":319281,"sourceType":"modelInstanceVersion","modelInstanceId":269395,"modelId":290383},{"sourceId":319482,"sourceType":"modelInstanceVersion","modelInstanceId":269572,"modelId":290559},{"sourceId":332314,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278557,"modelId":299461}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\n\n# CustomDataset: Loads HR images from nested subfolders and corresponding LR images from a given subfolder.\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, lr_subfolder_relative_path, transform_hr=None, transform_lr=None):\n        self.hr_images = []\n        for dirpath, _, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith('.png') and 'x4' not in file:\n                    self.hr_images.append(os.path.join(dirpath, file))\n        # Sort based on numeric order of filenames (assumes filenames are numbers)\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        \n        lr_root = os.path.join(root_dir, lr_subfolder_relative_path)\n        self.lr_images = []\n        for dirpath, _, files in os.walk(lr_root):\n            for file in files:\n                if file.endswith('.png') and 'x4' in file:\n                    self.lr_images.append(os.path.join(dirpath, file))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n        \n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n            \n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\n# SeparateDirsDataset: Loads HR and LR images from two separate directories.\nclass SeparateDirsDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.lr_images = [os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\n# Define transforms.\n# For x4 SR, we assume HR images are resized to 64x64 and LR images to 16x16.\ntransform_hr = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\ntransform_lr = transforms.Compose([\n    transforms.Resize((16, 16)),\n    transforms.ToTensor(),\n])\n\n# Set your dataset paths (update these if necessary):\nroot_directory = \"/kaggle/input/lsdir-hr\"             # For CustomDataset\nlr_relative_path = \"/kaggle/input/lsdir-hr/train_x4\"     # For CustomDataset\nhr_directory = \"/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR\"  # For SeparateDirsDataset\nlr_directory = \"/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4\"  # For SeparateDirsDataset\n\n# Create dataset objects.\ndataset_nested = CustomDataset(root_dir=root_directory,\n                               lr_subfolder_relative_path=lr_relative_path,\n                               transform_hr=transform_hr,\n                               transform_lr=transform_lr)\ndataset_separate = SeparateDirsDataset(hr_dir=hr_directory,\n                                       lr_dir=lr_directory,\n                                       transform_hr=transform_hr,\n                                       transform_lr=transform_lr)\n\n# Combine datasets.\ncombined_dataset = ConcatDataset([dataset_nested, dataset_separate])\nprint(f\"Total images: {len(combined_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:17:51.604838Z","iopub.execute_input":"2025-04-11T07:17:51.605033Z","iopub.status.idle":"2025-04-11T07:26:37.852154Z","shell.execute_reply.started":"2025-04-11T07:17:51.605013Z","shell.execute_reply":"2025-04-11T07:26:37.851414Z"}},"outputs":[{"name":"stdout","text":"Total images: 87641\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torchvision.transforms as T\nimport os\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\n\nval_transform_hr = T.Compose([\n    T.Resize((128, 128)),  # force same HR size for every image\n    T.ToTensor(),\n])\n\nclass ValDownsampleDataset(Dataset):\n    def __init__(self, hr_dir, transform_hr=None):\n        self.hr_dir = hr_dir\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.transform_hr = transform_hr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n\n        # 1) Transform HR to a fixed size.\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)  \n        else:\n            hr = T.ToTensor()(hr_img)\n\n        # 2) Create LR by x4 downsampling from *that transformed HR* (128×128 -> 32×32).\n        #    Do NOT use the original hr_img.size here. Instead, use hr.size().\n        c, h, w = hr.shape  # e.g. h=128, w=128\n        lr_width, lr_height = w // 4, h // 4\n\n        # Convert hr back to PIL for downsampling\n        hr_pil = T.ToPILImage()(hr)\n        lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n        lr = T.ToTensor()(lr_pil)\n\n        return {'hr': hr, 'lr': lr}\n\n# Then use `val_transform_hr` when creating the dataset.\nval_hr_directory = \"/kaggle/input/div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\"\nval_dataset_1 = ValDownsampleDataset(\n    hr_dir=val_hr_directory,\n    transform_hr=val_transform_hr  # now all HR images become exactly (3,128,128)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:26:37.853047Z","iopub.execute_input":"2025-04-11T07:26:37.853530Z","iopub.status.idle":"2025-04-11T07:26:37.872311Z","shell.execute_reply.started":"2025-04-11T07:26:37.853497Z","shell.execute_reply":"2025-04-11T07:26:37.871695Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\n\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, ConcatDataset\n\n# Re-define Dataset Loaders (if not already in the same script)\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, lr_subfolder_relative_path, transform_hr=None, transform_lr=None):\n        self.hr_images = []\n        for dirpath, _, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith('.png') and 'x4' not in file:\n                    self.hr_images.append(os.path.join(dirpath, file))\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n\n        lr_root = os.path.join(root_dir, lr_subfolder_relative_path)\n        self.lr_images = []\n        for dirpath, _, files in os.walk(lr_root):\n            for file in files:\n                if file.endswith('.png') and 'x4' in file:\n                    self.lr_images.append(os.path.join(dirpath, file))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n\n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\nclass SeparateDirsDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.lr_images = [os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\nclass ValDownsampleDataset(Dataset):\n    def __init__(self, hr_dir, transform_hr=None):\n        self.hr_dir = hr_dir\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.transform_hr = transform_hr\n        self.to_pil = transforms.ToPILImage()\n        self.to_tensor = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = self.to_tensor(hr_img)\n\n        c, h, w = hr.shape\n        lr_width, lr_height = w // 4, h // 4\n\n        hr_pil = self.to_pil(hr)\n        lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n        lr = self.to_tensor(lr_pil)\n\n        return {'hr': hr, 'lr': lr}\n\n# Channel Attention Module (Reused)\nclass ChannelAttention(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(num_channels, num_channels // reduction_ratio, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(num_channels // reduction_ratio, num_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, height, width = x.size()\n        out = self.global_avg_pool(x)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        weight = self.sigmoid(out)\n        return x * weight\n\n# Simplified Spatial Attention Module (Convolution-based)\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return self.sigmoid(out)\n\n# Hybrid Attention Block (Simplified)\nclass HybridAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16, spatial_kernel_size=7):\n        super(HybridAttentionBlock, self).__init__()\n        self.channel_attention = ChannelAttention(num_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(spatial_kernel_size)\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out_ca = self.channel_attention(out)\n        out_sa = self.spatial_attention(out)\n        out = out_ca * out_sa\n        return out + residual\n\n# HAT-Inspired Network\nclass HATInspiredNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_hab=10, scale_factor=4, reduction_ratio=16, spatial_kernel_size=7):\n        super(HATInspiredNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.hab_blocks = nn.Sequential(*[\n            HybridAttentionBlock(num_filters, reduction_ratio, spatial_kernel_size) for _ in range(num_hab)\n        ])\n        self.conv_after_hab = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        x = self.hab_blocks(x)\n        x = self.conv_after_hab(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Residual Channel Attention Block (RCAB)\nclass ResidualChannelAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ResidualChannelAttentionBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.ca = ChannelAttention(num_channels, reduction_ratio)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.ca(out)\n        return out + residual\n\n# Residual Group (RG)\nclass ResidualGroup(nn.Module):\n    def __init__(self, num_channels, num_rcab, reduction_ratio=16):\n        super(ResidualGroup, self).__init__()\n        layers = [ResidualChannelAttentionBlock(num_channels, reduction_ratio) for _ in range(num_rcab)]\n        self.rcabs = nn.Sequential(*layers)\n        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.rcabs(x)\n        out = self.conv(out)\n        return out + residual\n\n# RCAN-inspired Network\nclass RCANNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_rg=10, num_rcab=20, scale_factor=4, reduction_ratio=16):\n        super(RCANNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.residual_groups = nn.ModuleList([\n            ResidualGroup(num_filters, num_rcab, reduction_ratio) for _ in range(num_rg)\n        ])\n        self.conv_after_rg = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        for rg in self.residual_groups:\n            x = rg(x)\n        x = self.conv_after_rg(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Simplified SwinIR Block (Convolutional Approximation)\nclass SwinIRBlock(nn.Module):\n    def __init__(self, num_channels, window_size=8, reduction_ratio=4):\n        super(SwinIRBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.ca = ChannelAttention(num_channels, reduction_ratio)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.ca(out)\n        return out + residual\n\n# SwinIR-Inspired Network (Simplified)\nclass SwinIRInspiredNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_blocks=10, scale_factor=4, window_size=8):\n        super(SwinIRInspiredNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.swinir_blocks = nn.Sequential(*[\n            SwinIRBlock(num_filters, window_size) for _ in range(num_blocks)\n        ])\n        self.conv_after_blocks = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        x = self.swinir_blocks(x)\n        x = self.conv_after_blocks(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Function to calculate PSNR (Reused)\ndef calculate_psnr(img1, img2):\n    img1 = img1.mul(255).byte().cpu().numpy()\n    img2 = img2.mul(255).byte().cpu().numpy()\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 255.0\n    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n\ndef load_checkpoint(model, optimizer, scheduler, path, device):\n    if os.path.exists(path):\n        checkpoint = torch.load(path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        epoch = checkpoint['epoch']\n        best_psnr_individual = checkpoint['best_psnr']\n        print(f\"Loaded checkpoint from {path} at epoch {epoch} with best PSNR: {best_psnr_individual:.4f}\")\n\n        # Move optimizer state to the correct device\n        for state in optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor):\n                    state[k] = v.to(device)\n\n        return epoch, best_psnr_individual\n    return -1, -1\n\ndef train_ensemble_model(train_dataloader, val_dataloader, hat_model, rcan_model, swinir_model, criterion, hat_optimizer, rcan_optimizer, swinir_optimizer, hat_scheduler, rcan_scheduler, swinir_scheduler, num_epochs, device, save_dir=\"ensemble_checkpoints\"):\n    os.makedirs(save_dir, exist_ok=True)\n    best_psnr = 0.0\n    start_epoch = 0\n    checkpoint_path = os.path.join(save_dir, \"best_ensemble_model.pth\")\n\n    hat_checkpoint_path = os.path.join(\"hat_checkpoints\", \"best_model.pth\")\n    rcan_checkpoint_path = os.path.join(\"/kaggle/input/rcan/pytorch/default/1/best_model (2).pth\")\n    swinir_checkpoint_path = os.path.join(\"swinir_checkpoints\", \"best_model.pth\")\n\n    load_checkpoint(hat_model, hat_optimizer, hat_scheduler, hat_checkpoint_path, device)\n    load_checkpoint(rcan_model, rcan_optimizer, rcan_scheduler, rcan_checkpoint_path, device)\n    load_checkpoint(swinir_model, swinir_optimizer, swinir_scheduler, swinir_checkpoint_path, device)\n\n    hat_model.to(device)\n    rcan_model.to(device)\n    swinir_model.to(device)\n\n    print(\"Starting ensemble training.\")\n\n    for epoch in range(start_epoch, num_epochs):\n        hat_model.train()\n        rcan_model.train()\n        swinir_model.train()\n\n        train_loss = 0.0\n        train_psnr_sum = 0.0\n        num_train_batches = len(train_dataloader)\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train Ensemble]\")\n\n        for batch in train_progress_bar:\n            lr_images = batch['lr'].to(device)\n            hr_images = batch['hr'].to(device)\n\n            # Zero gradients for all optimizers\n            hat_optimizer.zero_grad()\n            rcan_optimizer.zero_grad()\n            swinir_optimizer.zero_grad()\n\n            # Forward passes\n            hat_output = hat_model(lr_images)\n            rcan_output = rcan_model(lr_images)\n            swinir_output = swinir_model(lr_images)\n\n            # Calculate individual losses\n            hat_loss = criterion(hat_output, hr_images)\n            rcan_loss = criterion(rcan_output, hr_images)\n            swinir_loss = criterion(swinir_output, hr_images)\n\n            # Ensemble loss (can be a simple average or a weighted sum)\n            total_loss = (hat_loss + rcan_loss + swinir_loss) / 3\n\n            # Backpropagate the total loss through all models\n            total_loss.backward()\n\n            # Update optimizers\n            hat_optimizer.step()\n            rcan_optimizer.step()\n            swinir_optimizer.step()\n\n            train_loss += total_loss.item() * lr_images.size(0)\n\n            # Calculate PSNR for the average output (optional, for monitoring)\n            avg_output = (hat_output + rcan_output + swinir_output) / 3\n            for i in range(avg_output.size(0)):\n                train_psnr_sum += calculate_psnr(avg_output[i].clamp(0, 1), hr_images[i])\n\n            train_progress_bar.set_postfix({'loss': f'{total_loss.item():.4f}'})\n\n        avg_train_loss = train_loss / len(train_dataloader.dataset)\n        avg_train_psnr = train_psnr_sum / len(train_dataloader.dataset)\n\n        # Validation\n        hat_model.eval()\n        rcan_model.eval()\n        swinir_model.eval()\n        val_psnr_sum = 0.0\n        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val Ensemble]\")\n\n        with torch.no_grad():\n            for batch in val_progress_bar:\n                lr_images = batch['lr'].to(device)\n                hr_images = batch['hr'].to(device)\n\n                hat_output = hat_model(lr_images)\n                rcan_output = rcan_model(lr_images)\n                swinir_output = swinir_model(lr_images)\n\n                avg_output = (hat_output + rcan_output + swinir_output) / 3\n                for i in range(avg_output.size(0)):\n                    val_psnr_sum += calculate_psnr(avg_output[i].clamp(0, 1), hr_images[i])\n\n        avg_val_psnr = val_psnr_sum / len(val_dataloader.dataset)\n\n        hat_scheduler.step()\n        rcan_scheduler.step()\n        swinir_scheduler.step()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Ensemble Train Loss: {avg_train_loss:.4f}, Ensemble Train PSNR (Avg): {avg_train_psnr:.4f}, Ensemble Val PSNR (Avg): {avg_val_psnr:.4f}\")\n\n        # Save the ensemble state (optional - you might prefer saving individual best models)\n        if avg_val_psnr > best_psnr:\n            best_psnr = avg_val_psnr\n            torch.save({\n                'epoch': epoch,\n                'hat_state_dict': hat_model.state_dict(),\n                'rcan_state_dict': rcan_model.state_dict(),\n                'swinir_state_dict': swinir_model.state_dict(),\n                'hat_optimizer_state_dict': hat_optimizer.state_dict(),\n                'rcan_optimizer_state_dict': rcan_optimizer.state_dict(),\n                'swinir_optimizer_state_dict': swinir_optimizer.state_dict(),\n                'hat_scheduler_state_dict': hat_scheduler.state_dict(),\n                'rcan_scheduler_state_dict': rcan_scheduler.state_dict(),\n                'swinir_scheduler_state_dict': swinir_scheduler.state_dict(),\n                'best_psnr': best_psnr\n            }, checkpoint_path)\n            print(f\"Ensemble Validation PSNR improved. Saved checkpoint to {checkpoint_path}\")\n\nif __name__ == '__main__':\n    # Hyperparameters\n    batch_size = 32\n    num_epochs = 200 # Adjust as needed for ensemble training\n    learning_rate = 1e-4\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize individual models\n    hat_model = HATInspiredNet(num_channels=3, num_filters=64, num_hab=10, scale_factor=4)\n    rcan_model = RCANNet(num_channels=3, num_filters=64, num_rg=10, num_rcab=20, scale_factor=4)\n    swinir_model = SwinIRInspiredNet(num_channels=3, num_filters=64, num_blocks=10, scale_factor=4)\n\n    # Initialize optimizers\n    hat_optimizer = optim.Adam(hat_model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n    rcan_optimizer = optim.Adam(rcan_model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n    swinir_optimizer = optim.Adam(swinir_model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n\n    # Initialize schedulers\n    hat_scheduler = optim.lr_scheduler.StepLR(hat_optimizer, step_size=100, gamma=0.5)\n    rcan_scheduler = optim.lr_scheduler.StepLR(rcan_optimizer, step_size=100, gamma=0.5)\n    swinir_scheduler = optim.lr_scheduler.StepLR(swinir_optimizer, step_size=100, gamma=0.5)\n\n    # Loss function\n    criterion = nn.L1Loss()\n\n    # Create data loaders (Reused)\n    train_dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset_1, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n    # Train the ensemble model\n    train_ensemble_model(train_dataloader, val_dataloader, hat_model, rcan_model, swinir_model, criterion, hat_optimizer, rcan_optimizer, swinir_optimizer, hat_scheduler, rcan_scheduler, swinir_scheduler, num_epochs, device, save_dir=\"ensemble_checkpoints\")\n\n    print(\"Ensemble training finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T09:56:46.787357Z","iopub.execute_input":"2025-04-11T09:56:46.787708Z","execution_failed":"2025-04-11T15:39:41.994Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-acae32b30940>:296: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(path)\n","output_type":"stream"},{"name":"stdout","text":"Loaded checkpoint from hat_checkpoints/best_model.pth at epoch 1 with best PSNR: 30.3371\nLoaded checkpoint from /kaggle/input/rcan/pytorch/default/1/best_model (2).pth at epoch 7 with best PSNR: 30.5619\nLoaded checkpoint from swinir_checkpoints/best_model.pth at epoch 1 with best PSNR: 30.3465\nStarting ensemble training.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:41<00:00,  1.36it/s, loss=0.0470]\nEpoch 1/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/200], Ensemble Train Loss: 0.0464, Ensemble Train PSNR (Avg): 30.4584, Ensemble Val PSNR (Avg): 30.5119\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:29<00:00,  1.36it/s, loss=0.0494]\nEpoch 2/200 [Val Ensemble]: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/200], Ensemble Train Loss: 0.0461, Ensemble Train PSNR (Avg): 30.4812, Ensemble Val PSNR (Avg): 30.5123\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:27<00:00,  1.36it/s, loss=0.0425]\nEpoch 3/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/200], Ensemble Train Loss: 0.0457, Ensemble Train PSNR (Avg): 30.5131, Ensemble Val PSNR (Avg): 30.5373\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:11<00:00,  1.38it/s, loss=0.0468]\nEpoch 4/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/200], Ensemble Train Loss: 0.0453, Ensemble Train PSNR (Avg): 30.5443, Ensemble Val PSNR (Avg): 30.5470\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:05<00:00,  1.38it/s, loss=0.0494]\nEpoch 5/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/200], Ensemble Train Loss: 0.0449, Ensemble Train PSNR (Avg): 30.5712, Ensemble Val PSNR (Avg): 30.5404\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:11<00:00,  1.38it/s, loss=0.0424]\nEpoch 6/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/200], Ensemble Train Loss: 0.0446, Ensemble Train PSNR (Avg): 30.5936, Ensemble Val PSNR (Avg): 30.5525\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:05<00:00,  1.38it/s, loss=0.0404]\nEpoch 7/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/200], Ensemble Train Loss: 0.0443, Ensemble Train PSNR (Avg): 30.6140, Ensemble Val PSNR (Avg): 30.5578\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:20<00:00,  1.37it/s, loss=0.0381]\nEpoch 8/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/200], Ensemble Train Loss: 0.0441, Ensemble Train PSNR (Avg): 30.6315, Ensemble Val PSNR (Avg): 30.5723\nEnsemble Validation PSNR improved. Saved checkpoint to ensemble_checkpoints/best_ensemble_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/200 [Train Ensemble]: 100%|██████████| 2739/2739 [33:13<00:00,  1.37it/s, loss=0.0429]\nEpoch 9/200 [Val Ensemble]: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/200], Ensemble Train Loss: 0.0438, Ensemble Train PSNR (Avg): 30.6475, Ensemble Val PSNR (Avg): 30.5722\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/200 [Train Ensemble]:  73%|███████▎  | 2012/2739 [24:41<08:32,  1.42it/s, loss=0.0432]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}