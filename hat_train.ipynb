{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056},{"sourceId":4853613,"sourceType":"datasetVersion","datasetId":2813430},{"sourceId":10972432,"sourceType":"datasetVersion","datasetId":6822545},{"sourceId":319281,"sourceType":"modelInstanceVersion","modelInstanceId":269395,"modelId":290383},{"sourceId":319482,"sourceType":"modelInstanceVersion","modelInstanceId":269572,"modelId":290559}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\n\n# CustomDataset: Loads HR images from nested subfolders and corresponding LR images from a given subfolder.\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, lr_subfolder_relative_path, transform_hr=None, transform_lr=None):\n        self.hr_images = []\n        for dirpath, _, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith('.png') and 'x4' not in file:\n                    self.hr_images.append(os.path.join(dirpath, file))\n        # Sort based on numeric order of filenames (assumes filenames are numbers)\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        \n        lr_root = os.path.join(root_dir, lr_subfolder_relative_path)\n        self.lr_images = []\n        for dirpath, _, files in os.walk(lr_root):\n            for file in files:\n                if file.endswith('.png') and 'x4' in file:\n                    self.lr_images.append(os.path.join(dirpath, file))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n        \n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n            \n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\n# SeparateDirsDataset: Loads HR and LR images from two separate directories.\nclass SeparateDirsDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.lr_images = [os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.lr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path).replace('x4',''))[0]))\n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(\"The number of HR and LR images do not match!\")\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr = Image.open(self.hr_images[idx]).convert('RGB')\n        lr = Image.open(self.lr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr)\n        if self.transform_lr:\n            lr = self.transform_lr(lr)\n        return {'hr': hr, 'lr': lr}\n\n# Define transforms.\n# For x4 SR, we assume HR images are resized to 64x64 and LR images to 16x16.\ntransform_hr = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\ntransform_lr = transforms.Compose([\n    transforms.Resize((16, 16)),\n    transforms.ToTensor(),\n])\n\n# Set your dataset paths (update these if necessary):\nroot_directory = \"/kaggle/input/lsdir-hr\"             # For CustomDataset\nlr_relative_path = \"/kaggle/input/lsdir-hr/train_x4\"     # For CustomDataset\nhr_directory = \"/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR\"  # For SeparateDirsDataset\nlr_directory = \"/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4\"  # For SeparateDirsDataset\n\n# Create dataset objects.\ndataset_nested = CustomDataset(root_dir=root_directory,\n                               lr_subfolder_relative_path=lr_relative_path,\n                               transform_hr=transform_hr,\n                               transform_lr=transform_lr)\ndataset_separate = SeparateDirsDataset(hr_dir=hr_directory,\n                                       lr_dir=lr_directory,\n                                       transform_hr=transform_hr,\n                                       transform_lr=transform_lr)\n\n# Combine datasets.\ncombined_dataset = ConcatDataset([dataset_nested, dataset_separate])\nprint(f\"Total images: {len(combined_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:17:51.604838Z","iopub.execute_input":"2025-04-11T07:17:51.605033Z","iopub.status.idle":"2025-04-11T07:26:37.852154Z","shell.execute_reply.started":"2025-04-11T07:17:51.605013Z","shell.execute_reply":"2025-04-11T07:26:37.851414Z"}},"outputs":[{"name":"stdout","text":"Total images: 87641\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torchvision.transforms as T\nimport os\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\n\nval_transform_hr = T.Compose([\n    T.Resize((128, 128)),  # force same HR size for every image\n    T.ToTensor(),\n])\n\nclass ValDownsampleDataset(Dataset):\n    def __init__(self, hr_dir, transform_hr=None):\n        self.hr_dir = hr_dir\n        self.hr_images = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith('.png')]\n        self.hr_images.sort(key=lambda path: int(os.path.splitext(os.path.basename(path))[0]))\n        self.transform_hr = transform_hr\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n\n        # 1) Transform HR to a fixed size.\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)  \n        else:\n            hr = T.ToTensor()(hr_img)\n\n        # 2) Create LR by x4 downsampling from *that transformed HR* (128×128 -> 32×32).\n        #    Do NOT use the original hr_img.size here. Instead, use hr.size().\n        c, h, w = hr.shape  # e.g. h=128, w=128\n        lr_width, lr_height = w // 4, h // 4\n\n        # Convert hr back to PIL for downsampling\n        hr_pil = T.ToPILImage()(hr)\n        lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n        lr = T.ToTensor()(lr_pil)\n\n        return {'hr': hr, 'lr': lr}\n\n# Then use `val_transform_hr` when creating the dataset.\nval_hr_directory = \"/kaggle/input/div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\"\nval_dataset_1 = ValDownsampleDataset(\n    hr_dir=val_hr_directory,\n    transform_hr=val_transform_hr  # now all HR images become exactly (3,128,128)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:26:37.853047Z","iopub.execute_input":"2025-04-11T07:26:37.853530Z","iopub.status.idle":"2025-04-11T07:26:37.872311Z","shell.execute_reply.started":"2025-04-11T07:26:37.853497Z","shell.execute_reply":"2025-04-11T07:26:37.871695Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom torchvision.transforms import ToPILImage\nfrom PIL import Image\n\n# Channel Attention Module (Reused from RCAN)\nclass ChannelAttention(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(num_channels, num_channels // reduction_ratio, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(num_channels // reduction_ratio, num_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, height, width = x.size()\n        out = self.global_avg_pool(x)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        weight = self.sigmoid(out)\n        return x * weight\n\n# Simplified Spatial Attention Module (Convolution-based)\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return self.sigmoid(out)\n\n# Hybrid Attention Block (Simplified)\nclass HybridAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16, spatial_kernel_size=7):\n        super(HybridAttentionBlock, self).__init__()\n        self.channel_attention = ChannelAttention(num_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(spatial_kernel_size)\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out_ca = self.channel_attention(out)\n        out_sa = self.spatial_attention(out)\n        out = out_ca * out_sa\n        return out + residual\n\n# HAT-Inspired Network\nclass HATInspiredNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_hab=10, scale_factor=4, reduction_ratio=16, spatial_kernel_size=7):\n        super(HATInspiredNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.hab_blocks = nn.Sequential(*[\n            HybridAttentionBlock(num_filters, reduction_ratio, spatial_kernel_size) for _ in range(num_hab)\n        ])\n        self.conv_after_hab = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        x = self.hab_blocks(x)\n        x = self.conv_after_hab(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Function to calculate PSNR (Reused)\ndef calculate_psnr(img1, img2):\n    img1 = img1.mul(255).byte().cpu().numpy()\n    img2 = img2.mul(255).byte().cpu().numpy()\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 255.0\n    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n\n# Training Function (Reused with minor adjustments)\ndef train_model(train_dataloader, val_dataloader, model, criterion, optimizer, scheduler, num_epochs, device, save_dir=\"hat_checkpoints\"):\n    os.makedirs(save_dir, exist_ok=True)\n    best_psnr = 0.0\n    start_epoch = 0\n    checkpoint_path = os.path.join(save_dir, \"best_model.pth\")\n\n    # Load checkpoint if it exists\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_psnr = checkpoint['best_psnr']\n        print(f\"Resuming training from epoch {start_epoch} with best PSNR: {best_psnr:.4f}\")\n        model.to(device)\n    else:\n        model.to(device)\n        print(\"Starting training from scratch.\")\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_psnr_sum = 0.0\n        num_train_batches = len(train_dataloader)\n        train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n\n        for batch in train_progress_bar:\n            lr_images = batch['lr'].to(device)\n            hr_images = batch['hr'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(lr_images)\n            loss = criterion(outputs, hr_images)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * lr_images.size(0)\n\n            for i in range(outputs.size(0)):\n                train_psnr_sum += calculate_psnr(outputs[i].clamp(0, 1), hr_images[i])\n\n            train_progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        avg_train_loss = train_loss / len(train_dataloader.dataset)\n        avg_train_psnr = train_psnr_sum / len(train_dataloader.dataset)\n\n        model.eval()\n        val_psnr_sum = 0.0\n        num_val_batches = len(val_dataloader)\n        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n\n        with torch.no_grad():\n            for batch in val_progress_bar:\n                lr_images = batch['lr'].to(device)\n                hr_images = batch['hr'].to(device)\n                outputs = model(lr_images)\n\n                for i in range(outputs.size(0)):\n                    val_psnr_sum += calculate_psnr(outputs[i].clamp(0, 1), hr_images[i])\n\n        avg_val_psnr = val_psnr_sum / len(val_dataloader.dataset)\n        scheduler.step()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train PSNR: {avg_train_psnr:.4f}, Val PSNR: {avg_val_psnr:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        # Save the best model based on validation PSNR\n        if avg_val_psnr > best_psnr:\n            best_psnr = avg_val_psnr\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_psnr': best_psnr\n            }, checkpoint_path)\n            print(f\"Validation PSNR improved. Saved checkpoint to {checkpoint_path}\")\n\nif __name__ == '__main__':\n\n    # Hyperparameters for HAT-Inspired model\n    batch_size = 32\n    num_epochs = 400\n    learning_rate = 1e-4\n    num_filters = 64\n    num_hab = 10  # Number of Hybrid Attention Blocks\n    scale_factor = 4\n    reduction_ratio = 16\n    spatial_kernel_size = 7\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    save_directory = \"hat_checkpoints\"\n\n    # Create data loaders (Reused)\n    train_dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_dataloader = DataLoader(val_dataset_1, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n    # Initialize model, loss, and optimizer\n    model = HATInspiredNet(num_channels=3, num_filters=num_filters, num_hab=num_hab, scale_factor=scale_factor, reduction_ratio=reduction_ratio, spatial_kernel_size=spatial_kernel_size)\n    criterion = nn.L1Loss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n\n    # Train the model\n    train_model(train_dataloader, val_dataloader, model, criterion, optimizer, scheduler, num_epochs, device, save_dir=save_directory)\n\n    print(\"Training finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:29:52.538699Z","iopub.execute_input":"2025-04-11T07:29:52.539060Z"}},"outputs":[{"name":"stdout","text":"Starting training from scratch.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/400 [Train]: 100%|██████████| 2739/2739 [31:22<00:00,  1.45it/s, loss=0.0484]\nEpoch 1/400 [Val]: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/400], Train Loss: 0.0562, Train PSNR: 29.8729, Val PSNR: 30.2458, LR: 0.000100\nValidation PSNR improved. Saved checkpoint to hat_checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/400 [Train]: 100%|██████████| 2739/2739 [29:45<00:00,  1.53it/s, loss=0.0442]\nEpoch 2/400 [Val]: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/400], Train Loss: 0.0495, Train PSNR: 30.1667, Val PSNR: 30.3371, LR: 0.000100\nValidation PSNR improved. Saved checkpoint to hat_checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/400 [Train]:  33%|███▎      | 911/2739 [09:56<22:17,  1.37it/s, loss=0.0457]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}