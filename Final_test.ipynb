{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":562453,"sourceType":"datasetVersion","datasetId":271102},{"sourceId":588358,"sourceType":"datasetVersion","datasetId":286056},{"sourceId":2869701,"sourceType":"datasetVersion","datasetId":1757438},{"sourceId":7034836,"sourceType":"datasetVersion","datasetId":4046929},{"sourceId":331060,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":277741,"modelId":298637},{"sourceId":332005,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278312,"modelId":299213},{"sourceId":332016,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278323,"modelId":299225},{"sourceId":332027,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278329,"modelId":299231},{"sourceId":332040,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278340,"modelId":299242},{"sourceId":332046,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278345,"modelId":299248},{"sourceId":332051,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278349,"modelId":299252},{"sourceId":332062,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278357,"modelId":299260},{"sourceId":332845,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278910,"modelId":299816}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n# (PSNR and EDSR definitions remain the same)\ndef calculate_psnr(sr, hr, max_val=1.0):\n    mse = F.mse_loss(sr, hr)\n    if mse == 0:\n        return 100\n    psnr = 10 * torch.log10((max_val ** 2) / mse)\n    return psnr.item()\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, n_feats, res_scale=0.1):\n        super(ResidualBlock, self).__init__()\n        self.res_scale = res_scale\n        self.conv1 = nn.Conv2d(n_feats, n_feats, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(n_feats, n_feats, kernel_size=3, padding=1)\n    def forward(self, x):\n        res = self.conv1(x)\n        res = self.relu(res)\n        res = self.conv2(res)\n        return x + res * self.res_scale\nclass EDSR(nn.Module):\n    def __init__(self, scale=4, n_resblocks=32, n_feats=64, res_scale=0.1, in_channels=3):\n        super(EDSR, self).__init__()\n        self.scale = scale\n        self.conv_in = nn.Conv2d(in_channels, n_feats, kernel_size=3, padding=1)\n        self.res_blocks = nn.Sequential(*[ResidualBlock(n_feats, res_scale) for _ in range(n_resblocks)])\n        self.conv_mid = nn.Conv2d(n_feats, n_feats, kernel_size=3, padding=1)\n        upscaling = []\n        if scale in [2, 3]:\n            upscaling.append(nn.Conv2d(n_feats, n_feats * (scale ** 2), kernel_size=3, padding=1))\n            upscaling.append(nn.PixelShuffle(scale))\n        elif scale == 4:\n            for _ in range(2):\n                upscaling.append(nn.Conv2d(n_feats, n_feats * 4, kernel_size=3, padding=1))\n                upscaling.append(nn.PixelShuffle(2))\n        else:\n            raise NotImplementedError(\"Scale factor {} not supported.\".format(scale))\n        self.upscale = nn.Sequential(*upscaling)\n        self.conv_out = nn.Conv2d(n_feats, in_channels, kernel_size=3, padding=1)\n    def forward(self, x):\n        x = self.conv_in(x)\n        residual = x\n        x = self.res_blocks(x)\n        x = self.conv_mid(x)\n        x = x + residual\n        x = self.upscale(x)\n        x = self.conv_out(x)\n        return x\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n            sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1] for metrics\n\n            # Ensure HR has the same size as SR for PSNR calculation\n            if hr.size()[-2:] != sr.size()[-2:]:\n                hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR\n            psnr = calculate_psnr(sr, hr)\n            total_psnr += psnr\n\n            # Calculate SSIM\n            total_ssim += ssim_metric(sr, hr).item()\n            num_batches += 1\n            pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the EDSR model\n    scale = 4\n    model = EDSR(scale=scale, n_resblocks=32, n_feats=64, res_scale=0.1, in_channels=3).to(device)\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/edsr_model/pytorch/default/1/EDSR_25.35.pth\"\n    model.load_state_dict(torch.load(pretrained_model_path, map_location=device)) # Load directly to the detected device\n    model.eval()\n\n    # --- Dataset Definitions (Ensure these are consistent with your previous code) ---\n    val_transform_hr = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n    val_transform_lr = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n\n    class ValDownsampleDataset(Dataset):\n        def __init__(self, hr_dir, transform_hr=None):\n            self.hr_dir = hr_dir\n            self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            self.transform_hr = transform_hr\n        def __len__(self):\n            return len(self.hr_images)\n        def __getitem__(self, idx):\n            hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n            if self.transform_hr:\n                hr = self.transform_hr(hr_img)\n            else:\n                hr = transforms.ToTensor()(hr_img)\n            c, h, w = hr.shape\n            lr_width, lr_height = w // 4, h // 4\n            hr_pil = transforms.ToPILImage()(hr)\n            lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n            lr = transforms.ToTensor()(lr_pil)\n            return {'hr': hr, 'lr': lr}\n\n    class PairedImageDataset(Dataset):\n        def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n            self.hr_dir = hr_dir\n            self.lr_dir = lr_dir\n            self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: os.path.splitext(os.path.basename(path))[0])\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: os.path.splitext(os.path.basename(path))[0])\n            if len(self.hr_images) != len(self.lr_images):\n                raise ValueError(f\"Number of HR images in {hr_dir} ({len(self.hr_images)}) and LR images in {lr_dir} ({len(self.lr_images)}) must be the same.\")\n            hr_basenames = [os.path.splitext(os.path.basename(path))[0] for path in self.hr_images]\n            lr_basenames = [os.path.splitext(os.path.basename(path))[0] for path in self.lr_images]\n            if hr_basenames != lr_basenames:\n                raise ValueError(f\"HR and LR image filenames in {hr_dir} and {lr_dir} must match.\")\n            self.transform_hr = transform_hr\n            self.transform_lr = transform_lr\n        def __len__(self):\n            return len(self.hr_images)\n        def __getitem__(self, idx):\n            hr_path = self.hr_images[idx]\n            lr_path = self.lr_images[idx]\n            hr_img = Image.open(hr_path).convert('RGB')\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_hr:\n                hr = self.transform_hr(hr_img)\n            else:\n                hr = transforms.ToTensor()(hr_img)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n            return {'hr': hr, 'lr': lr}\n\n    class Urban100PairedDataset(Dataset):\n        def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n            self.hr_dir = hr_dir\n            self.lr_dir = lr_dir\n            self.hr_images = sorted([f for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg')) and '_HR' in f])\n            self.lr_images = sorted([f for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg')) and '_LR' in f])\n            self.transform_hr = transform_hr\n            self.transform_lr = transform_lr\n            self.pairs = []\n            for hr_file in self.hr_images:\n                base_name = hr_file.replace('_HR', '')\n                lr_file = base_name.replace('.png', '_LR.png')\n                if lr_file in self.lr_images:\n                    hr_path = os.path.join(hr_dir, hr_file)\n                    lr_path = os.path.join(lr_dir, lr_file)\n                    self.pairs.append((hr_path, lr_path))\n        def __len__(self):\n            return len(self.pairs)\n        def __getitem__(self, idx):\n            hr_path, lr_path = self.pairs[idx]\n            hr_img = Image.open(hr_path).convert('RGB')\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_hr:\n                hr = self.transform_hr(hr_img)\n            else:\n                hr = transforms.ToTensor()(hr_img)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n            return {'hr': hr, 'lr': lr}\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n\n    # DIV2K\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDownsampleDataset(hr_dir=div2k_val_hr_directory, transform_hr=val_transform_hr)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_lr_dir_bicubic_4x = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/LR\")\n    if os.path.exists(bsd100_lr_dir_bicubic_4x) and len(os.listdir(bsd100_lr_dir_bicubic_4x)) > 0:\n        bsd100_dataset = PairedImageDataset(hr_dir=bsd100_hr_dir, lr_dir=bsd100_lr_dir_bicubic_4x, transform_hr=val_transform_hr, transform_lr=val_transform_lr)\n        print(f\"BSD100 Paired Dataset size: {len(bsd100_dataset)}\")\n        val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n    else:\n        bsd100_dataset = ValDownsampleDataset(hr_dir=bsd100_hr_dir, transform_hr=val_transform_hr)\n        print(f\"BSD100 Downsample Dataset size: {len(bsd100_dataset)}\")\n        val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDownsampleDataset(hr_dir=set14_hr_dir, transform_hr=val_transform_hr)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDownsampleDataset(hr_dir=set5_hr_dir, transform_hr=val_transform_hr)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_lr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/LOW x4 URban100\")\n    if os.path.exists(urban100_lr_dir) and len(os.listdir(urban100_lr_dir)) > 0:\n        urban100_dataset = Urban100PairedDataset(hr_dir=urban100_hr_dir, lr_dir=urban100_lr_dir, transform_hr=val_transform_hr, transform_lr=val_transform_lr)\n        print(f\"Urban100 Paired Dataset size: {len(urban100_dataset)}\")\n        val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n    else:\n        urban100_dataset = ValDownsampleDataset(hr_dir=urban100_hr_dir, transform_hr=val_transform_hr)\n        print(f\"Urban100 Downsample Dataset size: {len(urban100_dataset)}\")\n        val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- Validation Results (GPU) ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T06:39:20.437878Z","iopub.execute_input":"2025-04-11T06:39:20.438365Z","iopub.status.idle":"2025-04-11T06:39:40.491370Z","shell.execute_reply.started":"2025-04-11T06:39:20.438337Z","shell.execute_reply":"2025-04-11T06:39:40.490625Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2704371712.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device)) # Load directly to the detected device\n","output_type":"stream"},{"name":"stdout","text":"DIV2K Dataset size: 100\nBSD100 Paired Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Paired Dataset size: 100\n--- Validation Results (GPU) ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:08<00:00, 12.20it/s, psnr=21.14]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 23.26 dB, Avg SSIM: 0.6643\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:01<00:00, 48.35it/s, psnr=30.59]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 32.82 dB, Avg SSIM: 0.9123\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:00<00:00, 56.45it/s, psnr=19.80]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 23.13 dB, Avg SSIM: 0.6697\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00, 35.56it/s, psnr=23.69]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 24.75 dB, Avg SSIM: 0.7436\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:09<00:00, 10.79it/s, psnr=20.60]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 23.31 dB, Avg SSIM: 0.6748\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n#######################################\n# 2. PSNR Calculation Function (Reused)\n#######################################\n\ndef calculate_psnr(sr, hr, max_val=1.0):\n    mse = F.mse_loss(sr, hr)\n    if mse == 0:\n        return 100.0\n    psnr = 10 * torch.log10((max_val ** 2) / mse)\n    return psnr.item()\n\n#######################################\n# 4. ESRGAN Generator (Reused)\n#######################################\n\nclass ResidualDenseBlock(nn.Module):\n    def __init__(self, channels, growth_channels=32):\n        super(ResidualDenseBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, growth_channels, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(channels + growth_channels, growth_channels, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(channels + 2 * growth_channels, growth_channels, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(channels + 3 * growth_channels, growth_channels, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(channels + 4 * growth_channels, channels, kernel_size=3, padding=1)\n        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n        self.res_scale = 0.2\n\n    def forward(self, x):\n        x1 = self.lrelu(self.conv1(x))\n        x2 = self.lrelu(self.conv2(torch.cat([x, x1], 1)))\n        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], 1)))\n        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n        return x + x5 * self.res_scale\n\nclass RRDB(nn.Module):\n    def __init__(self, channels, growth_channels=32):\n        super(RRDB, self).__init__()\n        self.rdb1 = ResidualDenseBlock(channels, growth_channels)\n        self.rdb2 = ResidualDenseBlock(channels, growth_channels)\n        self.rdb3 = ResidualDenseBlock(channels, growth_channels)\n        self.res_scale = 0.2\n\n    def forward(self, x):\n        out = self.rdb1(x)\n        out = self.rdb2(out)\n        out = self.rdb3(out)\n        return x + out * self.res_scale\n\nclass ESRGANGenerator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, num_feat=64, num_rrdb=23, growth_channels=32, scale=4):\n        super(ESRGANGenerator, self).__init__()\n        self.conv_first = nn.Conv2d(in_channels, num_feat, kernel_size=3, padding=1)\n        rrdb_blocks = [RRDB(num_feat, growth_channels) for _ in range(num_rrdb)]\n        self.RRDB_trunk = nn.Sequential(*rrdb_blocks)\n        self.trunk_conv = nn.Conv2d(num_feat, num_feat, kernel_size=3, padding=1)\n        # Upsampling: for 4x scaling, use two PixelShuffle blocks.\n        upsample_layers = []\n        num_upsample = int(math.log(scale, 2))\n        for _ in range(num_upsample):\n            upsample_layers += [\n                nn.Conv2d(num_feat, num_feat * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.LeakyReLU(0.2, inplace=True)\n            ]\n        self.upsampling = nn.Sequential(*upsample_layers)\n        self.conv_last = nn.Conv2d(num_feat, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        fea = self.conv_first(x)\n        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n        fea = fea + trunk\n        out = self.upsampling(fea)\n        out = self.conv_last(out)\n        return out\n\n#######################################\n# Validation Function (Adapted from EDSR)\n#######################################\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n            sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1] for metrics\n\n            # Ensure HR has the same size as SR for PSNR calculation\n            if hr.size()[-2:] != sr.size()[-2:]:\n                hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR\n            psnr = calculate_psnr(sr, hr)\n            total_psnr += psnr\n\n            # Calculate SSIM\n            total_ssim += ssim_metric(sr, hr).item()\n            num_batches += 1\n            pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Reused and potentially adapted)\n#######################################\n\nval_transform_hr = transforms.Compose([\n    transforms.Resize((128, 128)), # Or your desired HR validation size\n    transforms.ToTensor(),\n])\nval_transform_lr = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nclass ValDownsampleDataset(Dataset):\n    def __init__(self, hr_dir, transform_hr=None, scale=4):\n        self.hr_dir = hr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        self.transform_hr = transform_hr\n        self.scale = scale\n    def __len__(self):\n        return len(self.hr_images)\n    def __getitem__(self, idx):\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n        c, h, w = hr.shape\n        lr_width, lr_height = w // self.scale, h // self.scale\n        hr_pil = transforms.ToPILImage()(hr)\n        lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n        lr = transforms.ToTensor()(lr_pil)\n        return {'hr': hr, 'lr': lr}\n\nclass PairedImageDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n        self.hr_dir = hr_dir\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: os.path.splitext(os.path.basename(path))[0])\n        self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: os.path.splitext(os.path.basename(path))[0])\n        if len(self.hr_images) != len(self.lr_images):\n            raise ValueError(f\"Number of HR images in {hr_dir} ({len(self.hr_images)}) and LR images in {lr_dir} ({len(self.lr_images)}) must be the same.\")\n        hr_basenames = [os.path.splitext(os.path.basename(path))[0] for path in self.hr_images]\n        lr_basenames = [os.path.splitext(os.path.basename(path))[0] for path in self.lr_images]\n        if hr_basenames != lr_basenames:\n            raise ValueError(f\"HR and LR image filenames in {hr_dir} and {lr_dir} must match.\")\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n    def __len__(self):\n        return len(self.hr_images)\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        lr_path = self.lr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n        lr_img = Image.open(lr_path).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n        if self.transform_lr:\n            lr = self.transform_lr(lr_img)\n        else:\n            lr = transforms.ToTensor()(lr_img)\n        return {'hr': hr, 'lr': lr}\n\nclass Urban100PairedDataset(Dataset):\n    def __init__(self, hr_dir, lr_dir, transform_hr=None, transform_lr=None):\n        self.hr_dir = hr_dir\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([f for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg')) and '_HR' in f])\n        self.lr_images = sorted([f for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg')) and '_LR' in f])\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.pairs = []\n        for hr_file in self.hr_images:\n            base_name = hr_file.replace('_HR', '')\n            lr_file = base_name.replace('.png', '_LR.png')\n            if lr_file in self.lr_images:\n                hr_path = os.path.join(hr_dir, hr_file)\n                lr_path = os.path.join(lr_dir, lr_file)\n                self.pairs.append((hr_path, lr_path))\n    def __len__(self):\n        return len(self.pairs)\n    def __getitem__(self, idx):\n        hr_path, lr_path = self.pairs[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n        lr_img = Image.open(lr_path).convert('RGB')\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n        if self.transform_lr:\n            lr = self.transform_lr(lr_img)\n        else:\n            lr = transforms.ToTensor()(lr_img)\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the ESRGAN model\n    scale = 4\n    model = ESRGANGenerator(scale=scale, num_rrdb=16, num_feat=64, growth_channels=32).to(device) # Use the same architecture as in training\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/esrgan_model/pytorch/default/1/ESRGAN_26.56.pth\" # Update this path to your pretrained model file\n    if os.path.exists(pretrained_model_path):\n        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        return\n\n    model.eval()\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n\n    # DIV2K (Using downsampling as the training data was generated this way)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDownsampleDataset(hr_dir=div2k_val_hr_directory, transform_hr=val_transform_hr, scale=scale)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100 (Using downsampling)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDownsampleDataset(hr_dir=bsd100_hr_dir, transform_hr=val_transform_hr, scale=scale)\n    print(f\"BSD100 Downsample Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14 (Using downsampling)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDownsampleDataset(hr_dir=set14_hr_dir, transform_hr=val_transform_hr, scale=scale)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5 (Using downsampling)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDownsampleDataset(hr_dir=set5_hr_dir, transform_hr=val_transform_hr, scale=scale)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100 (Using downsampling)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDownsampleDataset(hr_dir=urban100_hr_dir, transform_hr=val_transform_hr, scale=scale)\n    print(f\"Urban100 Downsample Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- ESRGAN Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T06:28:46.788641Z","iopub.execute_input":"2025-04-11T06:28:46.788980Z","iopub.status.idle":"2025-04-11T06:29:01.168775Z","shell.execute_reply.started":"2025-04-11T06:28:46.788953Z","shell.execute_reply":"2025-04-11T06:29:01.167807Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3579031161.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained model from: /kaggle/input/esrgan_model/pytorch/default/1/ESRGAN_26.56.pth\nDIV2K Dataset size: 100\nBSD100 Downsample Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Downsample Dataset size: 100\n--- ESRGAN Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:06<00:00, 15.04it/s, psnr=21.34]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 23.56 dB, Avg SSIM: 0.6803\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:02<00:00, 29.67it/s, psnr=22.74]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 25.46 dB, Avg SSIM: 0.7118\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:00<00:00, 25.46it/s, psnr=20.00]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 23.56 dB, Avg SSIM: 0.6911\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00, 18.88it/s, psnr=24.22]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 25.47 dB, Avg SSIM: 0.7762\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:03<00:00, 30.42it/s, psnr=17.01]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 21.83 dB, Avg SSIM: 0.6081\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n#######################################\n# 2. PSNR Calculation (Reused)\n#######################################\n\ndef calculate_psnr(sr, hr, max_val=1.0):\n    mse = F.mse_loss(sr, hr)\n    if mse == 0:\n        return 100.0\n    psnr = 10 * torch.log10((max_val ** 2) / mse)\n    return psnr.item()\n\n#######################################\n# 4. RGT-S Generator Architecture (Reused)\n#######################################\n\nclass RGTransformerBlock(nn.Module):\n    \"\"\"\n    An improved recursive-generalization transformer block.\n    It flattens spatial features into tokens, applies global self-attention\n    followed by a local self-attention branch, then an MLP.\n    \"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n        super(RGTransformerBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn_global = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.attn_local = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout)\n        self.norm3 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, int(dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Linear(int(dim * mlp_ratio), dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        # x: (B, C, H, W). Flatten spatial dims to (N, B, C) where N=H*W.\n        B, C, H, W = x.shape\n        N = H * W\n        x_flat = x.view(B, C, N).permute(2, 0, 1)  # shape: (N, B, C)\n\n        # Global self-attention\n        x_norm = self.norm1(x_flat)\n        attn_global, _ = self.attn_global(x_norm, x_norm, x_norm)\n        x_global = x_flat + attn_global\n\n        # Local self-attention (could use windowing; here we use full tokens for simplicity)\n        x_norm2 = self.norm2(x_global)\n        attn_local, _ = self.attn_local(x_norm2, x_norm2, x_norm2)\n        x_local = x_global + attn_local\n\n        # MLP block\n        x_norm3 = self.norm3(x_local)\n        mlp_out = self.mlp(x_norm3)\n        x_out = x_local + mlp_out\n\n        # Reshape back to (B, C, H, W)\n        x_out = x_out.permute(1, 2, 0).view(B, C, H, W)\n        return x_out\n\nclass RGT_S_Generator(nn.Module):\n    \"\"\"\n    Improved RGT-S Generator for 4x image super-resolution.\n    Architecture:\n      - Shallow convolutional embedding.\n      - A stack of transformer blocks (RGTransformerBlock) with increased capacity.\n      - A convolutional trunk (plus an extra refinement conv) with a skip connection.\n      - Upsampling via PixelShuffle.\n      - Final convolution to output the image.\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3, embed_dim=128, depth=12, num_heads=8, mlp_ratio=4.0, scale=4, dropout=0.1):\n        super(RGT_S_Generator, self).__init__()\n        self.conv_first = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)\n        self.transformer_blocks = nn.Sequential(*[\n            RGTransformerBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, dropout=dropout)\n            for _ in range(depth)\n        ])\n        # Convolutional trunk with extra refinement\n        self.trunk_conv = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n        self.refine_conv = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n\n        # Upsampling module: for 4x scaling, use two PixelShuffle blocks.\n        upsample_layers = []\n        num_upsample = int(math.log(scale, 2))\n        for _ in range(num_upsample):\n            upsample_layers += [\n                nn.Conv2d(embed_dim, embed_dim * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.GELU()\n            ]\n        self.upsampling = nn.Sequential(*upsample_layers)\n        self.conv_last = nn.Conv2d(embed_dim, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        fea = self.conv_first(x)\n        trans_out = self.transformer_blocks(fea)\n        trunk = self.trunk_conv(trans_out)\n        refined = self.refine_conv(trunk)\n        fea = fea + refined  # global skip connection\n        out = self.upsampling(fea)\n        out = self.conv_last(out)\n        return out\n\n#######################################\n# Validation Function (Reused)\n#######################################\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n            sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1] for metrics\n\n            # Ensure HR has the same size as SR for PSNR calculation\n            if hr.size()[-2:] != sr.size()[-2:]:\n                hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR\n            psnr = calculate_psnr(sr, hr)\n            total_psnr += psnr\n\n            # Calculate SSIM\n            total_ssim += ssim_metric(sr, hr).item()\n            num_batches += 1\n            pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Modified to limit HR size)\n#######################################\n\nval_transform_hr = transforms.Compose([\n    transforms.ToTensor(),\n])\nval_transform_lr = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nclass ValDownsampleDataset(Dataset):\n    def __init__(self, hr_dir, transform_hr=None, scale=4, max_hr_size=256): # Added max_hr_size\n        self.hr_dir = hr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        self.transform_hr = transform_hr\n        self.scale = scale\n        self.max_hr_size = max_hr_size # Maximum height or width for HR image\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n\n        # Resize HR image if it's larger than max_hr_size\n        if max(hr_img.size) > self.max_hr_size:\n            width, height = hr_img.size\n            if width > height:\n                new_width = self.max_hr_size\n                new_height = int(height * (new_width / width))\n            else:\n                new_height = self.max_hr_size\n                new_width = int(width * (new_height / height))\n            hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS) # Use LANCZOS for better quality\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n\n        c, h, w = hr.shape\n        lr_width, lr_height = w // self.scale, h // self.scale\n        hr_pil = transforms.ToPILImage()(hr)\n        lr_pil = hr_pil.resize((lr_width, lr_height), Image.BICUBIC)\n        lr = transforms.ToTensor()(lr_pil)\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the RGT-S Generator model\n    scale = 4\n    model = RGT_S_Generator(scale=scale, embed_dim=128, depth=12, num_heads=8, mlp_ratio=4.0, dropout=0.1).to(device) # Use the same architecture as in training\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/best_rgt/pytorch/default/1/RGT_S_27.57.pth\" # Updated path\n    if os.path.exists(pretrained_model_path):\n        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        return\n\n    model.eval()\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n\n    # DIV2K\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDownsampleDataset(hr_dir=div2k_val_hr_directory, transform_hr=val_transform_hr, scale=scale, max_hr_size=256) # Added max_hr_size\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDownsampleDataset(hr_dir=bsd100_hr_dir, transform_hr=val_transform_hr, scale=scale, max_hr_size=256) # Added max_hr_size\n    print(f\"BSD100 Downsample Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDownsampleDataset(hr_dir=set14_hr_dir, transform_hr=val_transform_hr, scale=scale, max_hr_size=256) # Added max_hr_size\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDownsampleDataset(hr_dir=set5_hr_dir, transform_hr=val_transform_hr, scale=scale, max_hr_size=256) # Added max_hr_size\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDownsampleDataset(hr_dir=urban100_hr_dir, transform_hr=val_transform_hr, scale=scale, max_hr_size=256) # Added max_hr_size\n    print(f\"Urban100 Downsample Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- RGT-S Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T06:46:51.181701Z","iopub.execute_input":"2025-04-11T06:46:51.181975Z","iopub.status.idle":"2025-04-11T06:48:14.744852Z","shell.execute_reply.started":"2025-04-11T06:46:51.181954Z","shell.execute_reply":"2025-04-11T06:48:14.744016Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/779479216.py:221: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n  _future_warning(\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained model from: /kaggle/input/best_rgt/pytorch/default/1/RGT_S_27.57.pth\nDIV2K Dataset size: 100\nBSD100 Downsample Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Downsample Dataset size: 100\n--- RGT-S Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s, psnr=20.17]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 22.27 dB, Avg SSIM: 0.6315\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:14<00:00,  5.44it/s, psnr=21.40]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 23.94 dB, Avg SSIM: 0.6482\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:04<00:00,  3.39it/s, psnr=18.71]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 22.82 dB, Avg SSIM: 0.6433\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:01<00:00,  2.78it/s, psnr=23.90]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 25.73 dB, Avg SSIM: 0.7561\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:22<00:00,  4.44it/s, psnr=17.29]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 20.25 dB, Avg SSIM: 0.5560\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\nimport torchvision.models as models\n\n################################################################################\n# MODEL: TRANSFORMER-BASED SR (Smaller RGT-like)\n################################################################################\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, dim, num_heads=4):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.query = nn.Linear(dim, dim)\n        self.key   = nn.Linear(dim, dim)\n        self.value = nn.Linear(dim, dim)\n        self.out   = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        B, N, C = x.size()\n        q = self.query(x).view(B, N, self.num_heads, self.head_dim)\n        k = self.key(x).view(B, N, self.num_heads, self.head_dim)\n        v = self.value(x).view(B, N, self.num_heads, self.head_dim)\n\n        q = q.permute(0, 2, 1, 3)\n        k = k.permute(0, 2, 1, 3)\n        v = v.permute(0, 2, 1, 3)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn, v)\n        out = out.permute(0, 2, 1, 3).contiguous().view(B, N, C)\n        out = self.out(out)\n        return out\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, expansion=2):\n        super(FeedForward, self).__init__()\n        hidden_dim = dim * expansion\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.fc2(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads=4, expansion=2):\n        super(TransformerBlock, self).__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = MultiHeadSelfAttention(dim, num_heads)\n        self.ln2 = nn.LayerNorm(dim)\n        self.ffn = FeedForward(dim, expansion)\n\n    def forward(self, x):\n        x_ = self.ln1(x)\n        x = x + self.attn(x_)\n        x_ = self.ln2(x)\n        x = x + self.ffn(x_)\n        return x\n\n\nclass ResidualGroup(nn.Module):\n    def __init__(self, dim, num_blocks=2, num_heads=4):\n        super(ResidualGroup, self).__init__()\n        blocks = []\n        for _ in range(num_blocks):\n            blocks.append(TransformerBlock(dim, num_heads=num_heads))\n        self.blocks = nn.Sequential(*blocks)\n        self.layer_norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        res = x\n        x = self.blocks(x)\n        x = self.layer_norm(x)\n        return x + res\n\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self, dim, scale=4):\n        super(UpsampleBlock, self).__init__()\n        self.conv = nn.Conv2d(dim, dim * (scale**2), kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(scale)\n        self.final_conv = nn.Conv2d(dim, 3, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.final_conv(x)\n        return x\n\n\nclass RGTNet(nn.Module):\n    \"\"\"\n    A smaller version of an RGT-like Transformer for 4× SR.\n    \"\"\"\n    def __init__(self, dim=48, num_groups=3, num_blocks=2, heads=4):\n        super(RGTNet, self).__init__()\n        self.shallow = nn.Conv2d(3, dim, kernel_size=3, padding=1)\n        self.groups = nn.ModuleList([\n            ResidualGroup(dim, num_blocks=num_blocks, num_heads=heads)\n            for _ in range(num_groups)\n        ])\n        self.ln = nn.LayerNorm(dim)\n        self.upsample = UpsampleBlock(dim, scale=4)\n\n    def forward(self, x):\n        fea = self.shallow(x)\n        B, C, H, W = fea.shape\n        fea_seq = fea.permute(0, 2, 3, 1).contiguous().view(B, H*W, C)\n\n        for g in self.groups:\n            fea_seq = g(fea_seq)\n\n        fea_seq = self.ln(fea_seq)\n        fea = fea_seq.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n        out = self.upsample(fea)\n        return out\n\n################################################################################\n# LOSS FUNCTIONS & METRICS - Reused PSNR\n################################################################################\n\ndef calculate_psnr(sr, hr, shave_border=4, only_y=True):\n    sr = sr.clamp(0,1)\n    hr = hr.clamp(0,1)\n    if shave_border > 0:\n        sr = sr[..., shave_border:-shave_border, shave_border:-shave_border]\n        hr = hr[..., shave_border:-shave_border, shave_border:-shave_border]\n    if only_y:\n        sr_y = rgb_to_y(sr)\n        hr_y = rgb_to_y(hr)\n        mse = F.mse_loss(sr_y, hr_y)\n    else:\n        mse = F.mse_loss(sr, hr)\n    if mse == 0:\n        return 999.0\n    return -10 * math.log10(mse.item())\n\ndef rgb_to_y(tensor_rgb):\n    # Weighted sum for Y channel\n    r = tensor_rgb[:,0,:,:]\n    g = tensor_rgb[:,1,:,:]\n    b = tensor_rgb[:,2,:,:]\n    y = 0.299*r + 0.587*g + 0.114*b\n    return y.unsqueeze(1)\n\n#######################################\n# Validation Function (Adapted with error handling)\n#######################################\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n\n            try:\n                sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1]\n\n                # Ensure HR has the same size as SR for PSNR calculation\n                if hr.size()[-2:] != sr.size()[-2:]:\n                    hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n                # Calculate PSNR (using the training script's function)\n                psnr = calculate_psnr(sr, hr, shave_border=4, only_y=True)\n                total_psnr += psnr\n\n                # Calculate SSIM (on RGB)\n                total_ssim += ssim_metric(sr, hr).item()\n                num_batches += 1\n                pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n            except Exception as e:\n                print(f\"Error during model forward pass: {e}\")\n                return -1, -1 # Indicate an error\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Adapted for direct LR/HR loading or downsampling)\n#######################################\n\nval_transform_hr = transforms.Compose([\n    transforms.ToTensor(),\n])\nval_transform_lr = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nclass ValDataset(Dataset):\n    def __init__(self, hr_dir, scale=4, transform_hr=None, transform_lr=None, load_lr_directly=False, lr_dir=None, max_hr_size=None): # Added max_hr_size\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.load_lr_directly = load_lr_directly\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        if load_lr_directly and lr_dir:\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            assert len(self.hr_images) == len(self.lr_images), \"Number of HR and LR images mismatch in direct load mode.\"\n        self.max_hr_size = max_hr_size\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n\n        if self.max_hr_size is not None and max(hr_img.size) > self.max_hr_size:\n            width, height = hr_img.size\n            if width > height:\n                new_width = self.max_hr_size\n                new_height = int(height * (new_width / width))\n            else:\n                new_height = self.max_hr_size\n                new_width = int(width * (new_height / height))\n            hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS)\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n\n        if self.load_lr_directly and self.lr_dir:\n            lr_path = self.lr_images[idx]\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n        else:\n            lr_width, lr_height = hr_img.width // self.scale, hr_img.height // self.scale\n            lr_img = hr_img.resize((lr_width, lr_height), Image.BICUBIC)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the RGTNet model (using the same parameters as in training)\n    model = RGTNet(dim=48, num_groups=3, num_blocks=2, heads=4).to(device)\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/sr_model/pytorch/default/1/SR_30.38.pth\" # Update this path if necessary\n    if os.path.exists(pretrained_model_path):\n        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        return\n\n    model.eval()\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n    scale = 4\n    max_hr_size = 256 # Keeping the maximum HR size\n\n    # DIV2K (HR only, will downsample)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDataset(hr_dir=div2k_val_hr_directory, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100 (HR only, will downsample)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDataset(hr_dir=bsd100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"BSD100 Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14 (HR only, will downsample)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDataset(hr_dir=set14_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5 (HR only, will downsample)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDataset(hr_dir=set5_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100 (HR only, will downsample)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDataset(hr_dir=urban100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Urban100 Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- RGTNet Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        if avg_psnr != -1:\n            print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T06:52:36.798737Z","iopub.execute_input":"2025-04-11T06:52:36.799617Z","iopub.status.idle":"2025-04-11T06:52:51.971613Z","shell.execute_reply.started":"2025-04-11T06:52:36.799584Z","shell.execute_reply":"2025-04-11T06:52:51.970456Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1821251105.py:289: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nLoaded pretrained model from: /kaggle/input/sr_model/pytorch/default/1/SR_30.38.pth\nDIV2K Dataset size: 100\nBSD100 Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Dataset size: 100\n--- RGTNet Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:07<00:00, 13.07it/s, psnr=20.19]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 22.48 dB, Avg SSIM: 0.6289\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:02<00:00, 31.58it/s, psnr=21.25]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 23.99 dB, Avg SSIM: 0.6408\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:00<00:00, 19.18it/s, psnr=18.46]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 23.34 dB, Avg SSIM: 0.6436\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00, 13.52it/s, psnr=23.86]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 26.20 dB, Avg SSIM: 0.7551\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:03<00:00, 26.77it/s, psnr=17.31]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 20.40 dB, Avg SSIM: 0.5514\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\nfrom torchvision.models import vgg19\n\n################################################################################\n# MODEL: SRResNet - Reused from training script\n################################################################################\n\ndef conv(in_c, out_c, k, s=1, p=0):\n    return nn.Conv2d(in_c, out_c, kernel_size=k, stride=s, padding=p)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = conv(channels, channels, 3, p=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv(channels, channels, 3, p=1)\n    def forward(self, x):\n        return x + self.conv2(self.relu(self.conv1(x)))\n\nclass SRResNet(nn.Module):\n    def __init__(self, num_blocks=16, upscale_factor=4):\n        super().__init__()\n        self.conv1 = conv(3, 64, 9, p=4)\n        self.relu = nn.ReLU(inplace=True)\n        self.res_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])\n        self.conv2 = conv(64, 64, 3, p=1)\n        self.upsample = nn.Sequential(\n            conv(64, 64*4, 3, p=1), nn.PixelShuffle(2), nn.ReLU(inplace=True),\n            conv(64, 64*4, 3, p=1), nn.PixelShuffle(2), nn.ReLU(inplace=True)\n        )\n        self.conv3 = conv(64, 3, 9, p=4)\n    def forward(self, x):\n        x1 = self.relu(self.conv1(x))\n        x2 = self.res_blocks(x1)\n        x = x1 + self.conv2(x2)\n        x = self.upsample(x)\n        return self.conv3(x)\n\n################################################################################\n# LOSS FUNCTIONS & METRICS - Reused PSNR\n################################################################################\n\ndef calc_psnr(sr, hr, max_val=1.0):\n    mse = nn.functional.mse_loss(sr, hr)\n    return 20 * math.log10(max_val) - 10 * math.log10(mse.item() + 1e-10)\n\n#######################################\n# Validation Function (Adapted)\n#######################################\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n            sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1]\n\n            # Ensure HR has the same size as SR for PSNR calculation\n            if hr.size()[-2:] != sr.size()[-2:]:\n                hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR (using the training script's function)\n            psnr = calc_psnr(sr, hr, max_val=1.0)\n            total_psnr += psnr\n\n            # Calculate SSIM (on RGB)\n            total_ssim += ssim_metric(sr, hr).item()\n            num_batches += 1\n            pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Adapted for direct LR/HR loading or downsampling)\n#######################################\n\nval_transform_hr = transforms.Compose([\n    transforms.ToTensor(),\n])\nval_transform_lr = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nclass ValDataset(Dataset):\n    def __init__(self, hr_dir, scale=4, transform_hr=None, transform_lr=None, load_lr_directly=False, lr_dir=None, max_hr_size=None): # Added max_hr_size\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.load_lr_directly = load_lr_directly\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        if load_lr_directly and lr_dir:\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            assert len(self.hr_images) == len(self.lr_images), \"Number of HR and LR images mismatch in direct load mode.\"\n        self.max_hr_size = max_hr_size\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n\n        if self.max_hr_size is not None and max(hr_img.size) > self.max_hr_size:\n            width, height = hr_img.size\n            if width > height:\n                new_width = self.max_hr_size\n                new_height = int(height * (new_width / width))\n            else:\n                new_height = self.max_hr_size\n                new_width = int(width * (new_height / height))\n            hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS)\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n\n        if self.load_lr_directly and self.lr_dir:\n            lr_path = self.lr_images[idx]\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n        else:\n            lr_width, lr_height = hr_img.width // self.scale, hr_img.height // self.scale\n            lr_img = hr_img.resize((lr_width, lr_height), Image.BICUBIC)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the SRResNet model (using the same parameters as in training)\n    model = SRResNet().to(device)\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/srresnet_model/pytorch/default/1/SRResnet_26.01.pth\" # Path from the training script\n    if os.path.exists(pretrained_model_path):\n        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        return\n\n    model.eval()\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n    scale = 4\n    max_hr_size = 512 # You can adjust this based on your GPU memory\n\n    # DIV2K (HR only, will downsample)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDataset(hr_dir=div2k_val_hr_directory, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100 (HR only, will downsample)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDataset(hr_dir=bsd100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"BSD100 Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14 (HR only, will downsample)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDataset(hr_dir=set14_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5 (HR only, will downsample)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDataset(hr_dir=set5_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100 (HR only, will downsample)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDataset(hr_dir=urban100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Urban100 Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- SRResNet Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        if avg_psnr != -1:\n            print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T06:54:25.487696Z","iopub.execute_input":"2025-04-11T06:54:25.488012Z","iopub.status.idle":"2025-04-11T06:54:42.073642Z","shell.execute_reply.started":"2025-04-11T06:54:25.487985Z","shell.execute_reply":"2025-04-11T06:54:42.072544Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded pretrained model from: /kaggle/input/srresnet_model/pytorch/default/1/SRResnet_26.01.pth\nDIV2K Dataset size: 100\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2764359273.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"BSD100 Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Dataset size: 100\n--- SRResNet Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:08<00:00, 12.00it/s, psnr=21.59]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 24.09 dB, Avg SSIM: 0.7027\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:02<00:00, 28.96it/s, psnr=22.46]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 25.62 dB, Avg SSIM: 0.7030\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:00<00:00, 20.62it/s, psnr=23.94]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 25.06 dB, Avg SSIM: 0.6999\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00, 21.59it/s, psnr=27.21]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 28.29 dB, Avg SSIM: 0.8169\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:04<00:00, 23.20it/s, psnr=21.03]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 21.61 dB, Avg SSIM: 0.6440\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport math\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nfrom tqdm import tqdm\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n##############################\n# 3. SwinIR Model Definition (Reused from training script)\n##############################\n\n# --- Helper Functions for Window Partitioning ---\n\ndef window_partition(x, window_size):\n    # x: (B, H, W, C)\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0,1,3,2,4,5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\ndef window_reverse(windows, window_size, H, W):\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0,1,3,2,4,5).contiguous().view(B, H, W, -1)\n    return x\n\n# --- Window Attention Module ---\n\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, window_size, num_heads):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # e.g. 8\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n        self.proj = nn.Linear(dim, dim)\n\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(((2 * window_size - 1) * (2 * window_size - 1), num_heads))\n        )\n        coords = torch.stack(torch.meshgrid(torch.arange(window_size), torch.arange(window_size)))\n        coords_flatten = torch.flatten(coords, 1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.permute(1,2,0).contiguous()\n        relative_coords[:,:,0] += window_size - 1\n        relative_coords[:,:,1] += window_size - 1\n        relative_coords[:,:,0] *= 2 * window_size - 1\n        relative_position_index = relative_coords.sum(-1)\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n\n    def forward(self, x):\n        # x: (num_windows*B, N, C) where N = window_size*window_size\n        B_, N, C = x.shape\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size * self.window_size, self.window_size * self.window_size, -1)\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).unsqueeze(0)\n        attn = attn + relative_position_bias\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v)\n        x = x.transpose(1,2).reshape(B_, N, C)\n        x = self.proj(x)\n        return x\n\n# --- Swin Transformer Block ---\n\nclass SwinTransformerBlock(nn.Module):\n    def __init__(self, dim, input_resolution, num_heads, window_size=8, shift_size=0, mlp_ratio=4.):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = WindowAttention(dim, window_size, num_heads)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, int(dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Linear(int(dim * mlp_ratio), dim)\n        )\n\n    def forward(self, x):\n        # x: (B, H, W, C)\n        B, H, W, C = x.shape\n        shortcut = x\n        x = self.norm1(x.view(B * H * W, C)).view(B, H, W, C)\n        if self.shift_size > 0:\n            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n        else:\n            shifted_x = x\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n        attn_windows = self.attn(x_windows)\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n        if self.shift_size > 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1,2))\n        else:\n            x = shifted_x\n        x = shortcut + x\n        x = x + self.mlp(self.norm2(x.view(B * H * W, C))).view(B, H, W, C)\n        return x\n\n# --- Residual Swin Transformer Block (RSTB) ---\n\nclass RSTB(nn.Module):\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size=8, mlp_ratio=4.):\n        super().__init__()\n        self.blocks = nn.Sequential(*[\n            SwinTransformerBlock(dim, input_resolution, num_heads, window_size,\n                                    shift_size=0 if (i % 2 == 0) else window_size // 2, mlp_ratio=mlp_ratio)\n            for i in range(depth)\n        ])\n        self.conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        res = x\n        B, C, H, W = x.shape\n        x = x.permute(0, 2, 3, 1).contiguous()\n        x = self.blocks(x)\n        x = x.permute(0, 3, 1, 2).contiguous()\n        return x + self.conv(res)\n\n# --- SwinIR Main Model ---\n\nclass SwinIR(nn.Module):\n    def __init__(self, upscale=4, in_channels=3, embed_dim=96, depths=[8,8,8,8],\n                 num_heads=[6,6,6,6], window_size=8, mlp_ratio=4.):\n        super(SwinIR, self).__init__()\n        self.upscale = upscale\n        self.conv_first = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)\n        self.feature_extraction = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n        # Assume HR crop size is 192x192.\n        input_resolution = (192, 192)\n        self.blocks = nn.ModuleList()\n        for i in range(len(depths)):\n            self.blocks.append(RSTB(dim=embed_dim, input_resolution=input_resolution,\n                                     depth=depths[i], num_heads=num_heads[i],\n                                     window_size=window_size, mlp_ratio=mlp_ratio))\n        self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)\n        if upscale == 4:\n            self.upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n                nn.Conv2d(embed_dim, embed_dim * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n            )\n        elif upscale == 2:\n            self.upsample = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim * 4, kernel_size=3, padding=1),\n                nn.PixelShuffle(2),\n            )\n        else:\n            raise NotImplementedError(f\"Upscale factor {upscale} not supported.\")\n        self.conv_last = nn.Conv2d(embed_dim, in_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.conv_first(x)\n        x = self.feature_extraction(x)\n        residual = x\n        for block in self.blocks:\n            x = block(x)\n        x = self.conv_after_body(x)\n        x = x + residual\n        x = self.upsample(x)\n        x = self.conv_last(x)\n        return x\n\n##############################\n# 2. PSNR Calculation Function (Reused from training script)\n##############################\n\ndef calculate_psnr(sr, hr, max_val=1.0):\n    mse = F.mse_loss(sr, hr)\n    if mse == 0:\n        return 100\n    psnr = 10 * torch.log10((max_val ** 2) / mse)\n    return psnr.item()\n\n#######################################\n# Validation Function (Adapted)\n#######################################\n\ndef validate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr = batch['hr'].to(device)\n            lr = batch['lr'].to(device)\n            sr = model(lr).clamp(0, 1) # Ensure output is in [0, 1]\n\n            # Ensure HR has the same size as SR for PSNR calculation\n            if hr.size()[-2:] != sr.size()[-2:]:\n                hr = F.interpolate(hr, size=sr.size()[-2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR (using the training script's function)\n            psnr = calculate_psnr(sr, hr, max_val=1.0)\n            total_psnr += psnr\n\n            # Calculate SSIM (on RGB)\n            total_ssim += ssim_metric(sr, hr).item()\n            num_batches += 1\n            pbar.set_postfix(psnr=f\"{psnr:.2f}\")\n\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Adapted for direct LR/HR loading or downsampling)\n#######################################\n\nval_transform_hr = transforms.Compose([\n    transforms.ToTensor(),\n])\nval_transform_lr = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nclass ValDataset(Dataset):\n    def __init__(self, hr_dir, scale=4, transform_hr=None, transform_lr=None, load_lr_directly=False, lr_dir=None, max_hr_size=None): # Added max_hr_size\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.load_lr_directly = load_lr_directly\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        if load_lr_directly and lr_dir:\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            assert len(self.hr_images) == len(self.lr_images), \"Number of HR and LR images mismatch in direct load mode.\"\n        self.max_hr_size = max_hr_size\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n        width, height = hr_img.size\n\n        # Resize HR image to the nearest multiple of 32\n        if self.max_hr_size is not None:\n            max_dim = max(width, height)\n            if max_dim > self.max_hr_size:\n                if width > height:\n                    new_width = (self.max_hr_size // 32) * 32\n                    new_height = int(height * (new_width / width))\n                    new_height = (new_height // 32) * 32\n                else:\n                    new_height = (self.max_hr_size // 32) * 32\n                    new_width = int(width * (new_height / height))\n                    new_width = (new_width // 32) * 32\n                hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS)\n                width, height = hr_img.size # Update width and height\n            elif width % 32 != 0 or height % 32 != 0:\n                new_width = (width // 32) * 32\n                new_height = (height // 32) * 32\n                if new_width == 0 or new_height == 0: # Handle cases where original size is smaller than 32\n                    new_width = max(32, width)\n                    new_height = max(32, height)\n                    new_width = (new_width // 32) * 32\n                    new_height = (new_height // 32) * 32\n\n                hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS)\n                width, height = hr_img.size # Update width and height\n        elif width % 32 != 0 or height % 32 != 0:\n            new_width = (width // 32) * 32\n            new_height = (height // 32) * 32\n            if new_width == 0 or new_height == 0: # Handle cases where original size is smaller than 32\n                new_width = max(32, width)\n                new_height = max(32, height)\n                new_width = (new_width // 32) * 32\n                new_height = (new_height // 32) * 32\n            hr_img = hr_img.resize((new_width, new_height), Image.LANCZOS)\n            width, height = hr_img.size # Update width and height\n\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = transforms.ToTensor()(hr_img)\n\n        if self.load_lr_directly and self.lr_dir:\n            lr_path = self.lr_images[idx]\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n        else:\n            lr_width, lr_height = width // self.scale, height // self.scale\n            lr_img = hr_img.resize((lr_width, lr_height), Image.BICUBIC)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = transforms.ToTensor()(lr_img)\n\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\ndef main():\n    # Define the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize the SwinIR model (using the same parameters as in training)\n    scale = 4\n    model = SwinIR(upscale=scale, in_channels=3, embed_dim=96, depths=[8,8,8,8],\n                   num_heads=[6,6,6,6], window_size=8, mlp_ratio=4.).to(device)\n\n    # Load the pretrained model weights\n    pretrained_model_path = \"/kaggle/input/swinir_model/pytorch/default/1/SWINIR_25.56.pth\" # Path from the user's output\n    if os.path.exists(pretrained_model_path):\n        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        return\n\n    model.eval()\n\n    # --- Load Validation Datasets ---\n    root_dir = \"/kaggle/input\"\n    val_datasets = {}\n    max_hr_size = 512 # You can adjust this based on your GPU memory\n\n    # DIV2K (HR only, will downsample)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDataset(hr_dir=div2k_val_hr_directory, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # BSD100 (HR only, will downsample)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDataset(hr_dir=bsd100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"BSD100 Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set14 (HR only, will downsample)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDataset(hr_dir=set14_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Set5 (HR only, will downsample)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDataset(hr_dir=set5_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    # Urban100 (HR only, will downsample)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDataset(hr_dir=urban100_hr_dir, scale=scale, transform_hr=val_transform_hr, max_hr_size=max_hr_size)\n    print(f\"Urban100 Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n    print(\"--- SwinIR Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(model, dataloader, device)\n        if avg_psnr != -1:\n            print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.2f} dB, Avg SSIM: {avg_ssim:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:00:34.485385Z","iopub.execute_input":"2025-04-11T07:00:34.486108Z","iopub.status.idle":"2025-04-11T07:01:04.799440Z","shell.execute_reply.started":"2025-04-11T07:00:34.486079Z","shell.execute_reply":"2025-04-11T07:01:04.798592Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded pretrained model from: /kaggle/input/swinir_model/pytorch/default/1/SWINIR_25.56.pth\nDIV2K Dataset size: 100\nBSD100 Dataset size: 80\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3411522389.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Set14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Dataset size: 100\n--- SwinIR Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:10<00:00,  9.88it/s, psnr=21.35]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 23.80 dB, Avg SSIM: 0.6829\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:07<00:00, 11.20it/s, psnr=22.16]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 25.30 dB, Avg SSIM: 0.6835\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:01<00:00,  9.03it/s, psnr=23.31]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 24.86 dB, Avg SSIM: 0.6859\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00, 11.66it/s, psnr=26.31]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 27.90 dB, Avg SSIM: 0.8006\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:10<00:00,  9.28it/s, psnr=20.50]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 21.33 dB, Avg SSIM: 0.6166\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom torchvision.transforms import ToPILImage, ToTensor, Compose\nfrom PIL import Image\nimport glob\nimport random\nimport math\nimport torch.nn.functional as F\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n# Channel Attention Module\nclass ChannelAttention(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(num_channels, num_channels // reduction_ratio, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(num_channels // reduction_ratio, num_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, height, width = x.size()\n        out = self.global_avg_pool(x)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        weight = self.sigmoid(out)\n        return x * weight\n\n# Residual Channel Attention Block (RCAB)\nclass ResidualChannelAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ResidualChannelAttentionBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.ca = ChannelAttention(num_channels, reduction_ratio)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.ca(out)\n        return out + residual\n\n# Residual Group (RG)\nclass ResidualGroup(nn.Module):\n    def __init__(self, num_channels, num_rcab, reduction_ratio=16):\n        super(ResidualGroup, self).__init__()\n        layers = [ResidualChannelAttentionBlock(num_channels, reduction_ratio) for _ in range(num_rcab)]\n        self.rcabs = nn.Sequential(*layers)\n        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.rcabs(x)\n        out = self.conv(out)\n        return out + residual\n\n# RCAN-inspired Network\nclass RCANNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_rg=10, num_rcab=20, scale_factor=4, reduction_ratio=16):\n        super(RCANNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.residual_groups = nn.ModuleList([\n            ResidualGroup(num_filters, num_rcab, reduction_ratio) for _ in range(num_rg)\n        ])\n        self.conv_after_rg = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        for rg in self.residual_groups:\n            x = rg(x)\n        x = self.conv_after_rg(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Function to calculate PSNR\ndef calculate_psnr(img1, img2):\n    img1 = img1.mul(255).byte().cpu().numpy()\n    img2 = img2.mul(255).byte().cpu().numpy()\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 255.0\n    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n\n#######################################\n# Validation Function\n#######################################\n\ndef validate_model(dataloader, model, device):\n    model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation\")\n        for batch in pbar:\n            hr_images = batch['hr'].to(device)\n            lr_images = batch['lr'].to(device)\n            outputs = model(lr_images).clamp(0, 1)\n\n            # Resize HR images to match the output size\n            resized_hr_images = F.interpolate(hr_images, size=outputs.shape[2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR and SSIM for the entire batch\n            current_batch_size = lr_images.size(0)\n            for i in range(current_batch_size):\n                total_psnr += calculate_psnr(outputs[i].cpu().float(), resized_hr_images[i].cpu().float())\n                total_ssim += ssim_metric(outputs[i].unsqueeze(0), resized_hr_images[i].unsqueeze(0)).item()\n            num_batches += current_batch_size\n\n    if num_batches == 0:\n        return -1, -1  # Return -1 to indicate no data was processed\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Using ValDataset from previous SwinIR script)\n#######################################\n\nval_transform_hr = Compose([\n    ToTensor(),\n])\nval_transform_lr = Compose([\n    ToTensor(),\n])\n\nclass ValDataset(Dataset):\n    def __init__(self, hr_dir, scale=4, transform_hr=None, transform_lr=None, load_lr_directly=False, lr_dir=None):\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.load_lr_directly = load_lr_directly\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        if load_lr_directly and lr_dir:\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            assert len(self.hr_images) == len(self.lr_images), \"Number of HR and LR images mismatch in direct load mode.\"\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = ToTensor()(hr_img)\n\n        if self.load_lr_directly and self.lr_dir:\n            lr_path = self.lr_images[idx]\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = ToTensor()(lr_img)\n        else:\n            width, height = hr_img.size\n            lr_width, lr_height = width // self.scale, height // self.scale\n            lr_img = hr_img.resize((lr_width, lr_height), Image.BICUBIC)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = ToTensor()(lr_img)\n\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Validation\n#######################################\n\nif __name__ == '__main__':\n    # Hyperparameters\n    batch_size = 1 # Using batch size 1 as requested\n    scale_factor = 4\n    num_filters = 64\n    num_rg = 10\n    num_rcab = 20\n    reduction_ratio = 16\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    pretrained_model_path = \"/kaggle/input/rcan_model/pytorch/default/1/best_model (2).pth\" # Updated path\n    root_dir = \"/kaggle/input\"\n\n    # Initialize model\n    model = RCANNet(num_channels=3, num_filters=num_filters, num_rg=num_rg, num_rcab=num_rcab, scale_factor=scale_factor, reduction_ratio=reduction_ratio)\n\n    # Load pretrained model weights\n    if os.path.exists(pretrained_model_path):\n        checkpoint = torch.load(pretrained_model_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.to(device)\n        print(f\"Loaded pretrained model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained model not found at: {pretrained_model_path}\")\n        exit()\n\n    # Define validation datasets and dataloaders\n    val_datasets = {}\n\n    # DIV2K (HR only, will downsample)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDataset(hr_dir=div2k_val_hr_directory, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\n    # BSD100 (HR only, will downsample)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDataset(hr_dir=bsd100_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"BSD100 Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Set14 (HR only, will downsample)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDataset(hr_dir=set14_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Set5 (HR only, will downsample)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDataset(hr_dir=set5_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Urban100 (HR only, will downsample)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDataset(hr_dir=urban100_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Urban100 Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Validate the model on all datasets\n    print(\"--- RCAN Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_model(dataloader, model, device)\n        if avg_psnr != -1:\n            print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.4f} dB, Avg SSIM: {avg_ssim:.4f}\")\n        else:\n            print(f\"Dataset: {dataset_name}, No data found for validation.\")\n\n    print(\"Validation finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T07:12:16.336607Z","iopub.execute_input":"2025-04-11T07:12:16.337509Z","iopub.status.idle":"2025-04-11T07:16:17.096237Z","shell.execute_reply.started":"2025-04-11T07:12:16.337478Z","shell.execute_reply":"2025-04-11T07:16:17.095154Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1369971015.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained model from: /kaggle/input/rcan_model/pytorch/default/1/best_model (2).pth\nDIV2K Dataset size: 100\nBSD100 Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Dataset size: 100\n--- RCAN Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [03:03<00:00,  1.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 33.1956 dB, Avg SSIM: 0.8091\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 80/80 [00:07<00:00, 10.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 31.6858 dB, Avg SSIM: 0.7306\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 14/14 [00:02<00:00,  6.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 31.5765 dB, Avg SSIM: 0.7370\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 5/5 [00:00<00:00,  7.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 32.3066 dB, Avg SSIM: 0.8417\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 100/100 [00:45<00:00,  2.19it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 31.2350 dB, Avg SSIM: 0.7392\nValidation finished!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom torchvision.transforms import ToPILImage, ToTensor, Compose\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchmetrics import StructuralSimilarityIndexMeasure\n\n# Channel Attention Module (Reused)\nclass ChannelAttention(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(num_channels, num_channels // reduction_ratio, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(num_channels // reduction_ratio, num_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, num_channels, height, width = x.size()\n        out = self.global_avg_pool(x)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.fc2(out)\n        weight = self.sigmoid(out)\n        return x * weight\n\n# Simplified Spatial Attention Module (Convolution-based)\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return self.sigmoid(out)\n\n# Hybrid Attention Block (Simplified)\nclass HybridAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16, spatial_kernel_size=7):\n        super(HybridAttentionBlock, self).__init__()\n        self.channel_attention = ChannelAttention(num_channels, reduction_ratio)\n        self.spatial_attention = SpatialAttention(spatial_kernel_size)\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out_ca = self.channel_attention(out)\n        out_sa = self.spatial_attention(out)\n        out = out_ca * out_sa\n        return out + residual\n\n# HAT-Inspired Network\nclass HATInspiredNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_hab=10, scale_factor=4, reduction_ratio=16, spatial_kernel_size=7):\n        super(HATInspiredNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.hab_blocks = nn.Sequential(*[\n            HybridAttentionBlock(num_filters, reduction_ratio, spatial_kernel_size) for _ in range(num_hab)\n        ])\n        self.conv_after_hab = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        x = self.hab_blocks(x)\n        x = self.conv_after_hab(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Residual Channel Attention Block (RCAB)\nclass ResidualChannelAttentionBlock(nn.Module):\n    def __init__(self, num_channels, reduction_ratio=16):\n        super(ResidualChannelAttentionBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.ca = ChannelAttention(num_channels, reduction_ratio)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.ca(out)\n        return out + residual\n\n# Residual Group (RG)\nclass ResidualGroup(nn.Module):\n    def __init__(self, num_channels, num_rcab, reduction_ratio=16):\n        super(ResidualGroup, self).__init__()\n        layers = [ResidualChannelAttentionBlock(num_channels, reduction_ratio) for _ in range(num_rcab)]\n        self.rcabs = nn.Sequential(*layers)\n        self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        out = self.rcabs(x)\n        out = self.conv(out)\n        return out + residual\n\n# RCAN-inspired Network\nclass RCANNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_rg=10, num_rcab=20, scale_factor=4, reduction_ratio=16):\n        super(RCANNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.residual_groups = nn.ModuleList([\n            ResidualGroup(num_filters, num_rcab, reduction_ratio) for _ in range(num_rg)\n        ])\n        self.conv_after_rg = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        for rg in self.residual_groups:\n            x = rg(x)\n        x = self.conv_after_rg(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Simplified SwinIR Block (Convolutional Approximation)\nclass SwinIRBlock(nn.Module):\n    def __init__(self, num_channels, window_size=8, reduction_ratio=4):\n        super(SwinIRBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.ca = ChannelAttention(num_channels, reduction_ratio)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.ca(out)\n        return out + residual\n\n# SwinIR-Inspired Network (Simplified)\nclass SwinIRInspiredNet(nn.Module):\n    def __init__(self, num_channels=3, num_filters=64, num_blocks=10, scale_factor=4, window_size=8):\n        super(SwinIRInspiredNet, self).__init__()\n        self.num_filters = num_filters\n        self.scale_factor = scale_factor\n        self.initial_conv = nn.Conv2d(num_channels, num_filters, kernel_size=3, padding=1)\n        self.swinir_blocks = nn.Sequential(*[\n            SwinIRBlock(num_filters, window_size) for _ in range(num_blocks)\n        ])\n        self.conv_after_blocks = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1)\n        self.upsample = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters * (scale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(scale_factor)\n        )\n        self.final_conv = nn.Conv2d(num_filters, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.initial_conv(x)\n        residual = x\n        x = self.swinir_blocks(x)\n        x = self.conv_after_blocks(x)\n        x += residual\n        x = self.upsample(x)\n        x = self.final_conv(x)\n        return x\n\n# Function to calculate PSNR (Reused)\ndef calculate_psnr(img1, img2):\n    img1 = img1.mul(255).byte().cpu().numpy()\n    img2 = img2.mul(255).byte().cpu().numpy()\n    mse = np.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 255.0\n    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n\n#######################################\n# Validation Function for Ensemble Model\n#######################################\n\ndef validate_ensemble_model(dataloader, hat_model, rcan_model, swinir_model, device):\n    hat_model.eval()\n    rcan_model.eval()\n    swinir_model.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    num_batches = 0\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Validation Ensemble\")\n        for batch in pbar:\n            hr_images = batch['hr'].to(device)\n            lr_images = batch['lr'].to(device)\n\n            hat_output = hat_model(lr_images).clamp(0, 1)\n            rcan_output = rcan_model(lr_images).clamp(0, 1)\n            swinir_output = swinir_model(lr_images).clamp(0, 1)\n\n            # Average the outputs of the three models\n            avg_output = (hat_output + rcan_output + swinir_output) / 3\n\n            # Resize HR images to match the output size\n            resized_hr_images = F.interpolate(hr_images, size=avg_output.shape[2:], mode='bicubic', align_corners=False)\n\n            # Calculate PSNR and SSIM for the entire batch\n            current_batch_size = lr_images.size(0)\n            for i in range(current_batch_size):\n                total_psnr += calculate_psnr(avg_output[i].cpu().float(), resized_hr_images[i].cpu().float())\n                total_ssim += ssim_metric(avg_output[i].unsqueeze(0), resized_hr_images[i].unsqueeze(0)).item()\n            num_batches += current_batch_size\n\n    if num_batches == 0:\n        return -1, -1  # Return -1 to indicate no data was processed\n    avg_psnr = total_psnr / num_batches\n    avg_ssim = total_ssim / num_batches\n    return avg_psnr, avg_ssim\n\n#######################################\n# Dataset Definitions (Reused)\n#######################################\n\nval_transform_hr = Compose([\n    ToTensor(),\n])\nval_transform_lr = Compose([\n    ToTensor(),\n])\n\nclass ValDataset(Dataset):\n    def __init__(self, hr_dir, scale=4, transform_hr=None, transform_lr=None, load_lr_directly=False, lr_dir=None):\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.transform_hr = transform_hr\n        self.transform_lr = transform_lr\n        self.load_lr_directly = load_lr_directly\n        self.lr_dir = lr_dir\n        self.hr_images = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n        if load_lr_directly and lr_dir:\n            self.lr_images = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))],\n                                    key=lambda path: int(os.path.splitext(os.path.basename(path))[0]) if os.path.splitext(os.path.basename(path))[0].isdigit() else os.path.splitext(os.path.basename(path))[0])\n            assert len(self.hr_images) == len(self.lr_images), \"Number of HR and LR images mismatch in direct load mode.\"\n\n    def __len__(self):\n        return len(self.hr_images)\n\n    def __getitem__(self, idx):\n        hr_path = self.hr_images[idx]\n        hr_img = Image.open(hr_path).convert('RGB')\n\n        if self.transform_hr:\n            hr = self.transform_hr(hr_img)\n        else:\n            hr = ToTensor()(hr_img)\n\n        if self.load_lr_directly and self.lr_dir:\n            lr_path = self.lr_images[idx]\n            lr_img = Image.open(lr_path).convert('RGB')\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = ToTensor()(lr_img)\n        else:\n            width, height = hr_img.size\n            lr_width, lr_height = width // self.scale, height // self.scale\n            lr_img = hr_img.resize((lr_width, lr_height), Image.BICUBIC)\n            if self.transform_lr:\n                lr = self.transform_lr(lr_img)\n            else:\n                lr = ToTensor()(lr_img)\n\n        return {'hr': hr, 'lr': lr}\n\n#######################################\n# Main Function for Ensemble Validation\n#######################################\n\nif __name__ == '__main__':\n    # Hyperparameters\n    batch_size = 1 # Using batch size 1 for validation\n    scale_factor = 4\n    num_filters = 64\n    num_hab = 10\n    reduction_ratio_hat = 16\n    spatial_kernel_size = 7\n    num_rg = 10\n    num_rcab = 20\n    reduction_ratio_rcan = 16\n    num_blocks_swinir = 10\n    window_size_swinir = 8\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    pretrained_model_path = \"/kaggle/input/ensemble_model_best/pytorch/default/1/best_ensemble_model (2).pth\" # Path to the saved ensemble checkpoint\n    root_dir = \"/kaggle/input\"\n\n    # Initialize individual models\n    hat_model = HATInspiredNet(num_channels=3, num_filters=num_filters, num_hab=num_hab, scale_factor=scale_factor, reduction_ratio=reduction_ratio_hat, spatial_kernel_size=spatial_kernel_size)\n    rcan_model = RCANNet(num_channels=3, num_filters=num_filters, num_rg=num_rg, num_rcab=num_rcab, scale_factor=scale_factor, reduction_ratio=reduction_ratio_rcan)\n    swinir_model = SwinIRInspiredNet(num_channels=3, num_filters=num_filters, num_blocks=num_blocks_swinir, scale_factor=scale_factor, window_size=window_size_swinir)\n\n    # Load pretrained ensemble model weights\n    if os.path.exists(pretrained_model_path):\n        checkpoint = torch.load(pretrained_model_path, map_location=device)\n        hat_model.load_state_dict(checkpoint['hat_state_dict'])\n        rcan_model.load_state_dict(checkpoint['rcan_state_dict'])\n        swinir_model.load_state_dict(checkpoint['swinir_state_dict'])\n        hat_model.to(device)\n        rcan_model.to(device)\n        swinir_model.to(device)\n        print(f\"Loaded pretrained ensemble model from: {pretrained_model_path}\")\n    else:\n        print(f\"Pretrained ensemble model not found at: {pretrained_model_path}\")\n        exit()\n\n    # Define validation datasets and dataloaders\n    val_datasets = {}\n\n    # DIV2K (HR only, will downsample)\n    div2k_val_hr_directory = os.path.join(root_dir, \"div2k-dataset/DIV2K_valid_HR/DIV2K_valid_HR\")\n    div2k_dataset = ValDataset(hr_dir=div2k_val_hr_directory, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"DIV2K Dataset size: {len(div2k_dataset)}\")\n    val_datasets['DIV2K'] = DataLoader(div2k_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # BSD100 (HR only, will downsample)\n    bsd100_hr_dir = os.path.join(root_dir, \"bsd100/bsd100/bicubic_4x/train/HR\")\n    bsd100_dataset = ValDataset(hr_dir=bsd100_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"BSD100 Dataset size: {len(bsd100_dataset)}\")\n    val_datasets['BSD100'] = DataLoader(bsd100_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Set14 (HR only, will downsample)\n    set14_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set14/Set14\")\n    set14_dataset = ValDataset(hr_dir=set14_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Set14 Dataset size: {len(set14_dataset)}\")\n    val_datasets['Set14'] = DataLoader(set14_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Set5 (HR only, will downsample)\n    set5_hr_dir = os.path.join(root_dir, \"set-5-14-super-resolution-dataset/Set5/Set5\")\n    set5_dataset = ValDataset(hr_dir=set5_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Set5 Dataset size: {len(set5_dataset)}\")\n    val_datasets['Set5'] = DataLoader(set5_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Urban100 (HR only, will downsample)\n    urban100_hr_dir = os.path.join(root_dir, \"urban100/Urban 100/X4 Urban100/X4/HIGH x4 URban100\")\n    urban100_dataset = ValDataset(hr_dir=urban100_hr_dir, scale=scale_factor, transform_hr=val_transform_hr)\n    print(f\"Urban100 Dataset size: {len(urban100_dataset)}\")\n    val_datasets['Urban100'] = DataLoader(urban100_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # Validate the ensemble model on all datasets\n    print(\"--- Ensemble Model Validation Results ---\")\n    for dataset_name, dataloader in val_datasets.items():\n        avg_psnr, avg_ssim = validate_ensemble_model(dataloader, hat_model, rcan_model, swinir_model, device)\n        if avg_psnr != -1:\n            print(f\"Dataset: {dataset_name}, Avg PSNR: {avg_psnr:.4f} dB, Avg SSIM: {avg_ssim:.4f}\")\n        else:\n            print(f\"Dataset: {dataset_name}, No data found for validation.\")\n\n    print(\"Ensemble validation finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T16:23:58.384745Z","iopub.execute_input":"2025-04-11T16:23:58.385422Z","iopub.status.idle":"2025-04-11T16:29:33.902681Z","shell.execute_reply.started":"2025-04-11T16:23:58.385398Z","shell.execute_reply":"2025-04-11T16:29:33.901616Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/76444682.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(pretrained_model_path, map_location=device)\n/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n  _future_warning(\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained ensemble model from: /kaggle/input/ensemble_model_best/pytorch/default/1/best_ensemble_model (2).pth\nDIV2K Dataset size: 100\nBSD100 Dataset size: 80\nSet14 Dataset size: 14\nSet5 Dataset size: 5\nUrban100 Dataset size: 100\n--- Ensemble Model Validation Results ---\n","output_type":"stream"},{"name":"stderr","text":"Validation Ensemble: 100%|██████████| 100/100 [04:05<00:00,  2.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: DIV2K, Avg PSNR: 33.2126 dB, Avg SSIM: 0.8099\n","output_type":"stream"},{"name":"stderr","text":"Validation Ensemble: 100%|██████████| 80/80 [00:08<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: BSD100, Avg PSNR: 31.6892 dB, Avg SSIM: 0.7309\n","output_type":"stream"},{"name":"stderr","text":"Validation Ensemble: 100%|██████████| 14/14 [00:02<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set14, Avg PSNR: 31.6018 dB, Avg SSIM: 0.7385\n","output_type":"stream"},{"name":"stderr","text":"Validation Ensemble: 100%|██████████| 5/5 [00:00<00:00,  7.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset: Set5, Avg PSNR: 32.3534 dB, Avg SSIM: 0.8446\n","output_type":"stream"},{"name":"stderr","text":"Validation Ensemble: 100%|██████████| 100/100 [01:02<00:00,  1.61it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset: Urban100, Avg PSNR: 31.2291 dB, Avg SSIM: 0.7383\nEnsemble validation finished!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}